{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b9e9c9",
   "metadata": {},
   "source": [
    "# Project Lighthouse "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b99ff7",
   "metadata": {},
   "source": [
    "#### Timeline: Feb 2022 - Nov 2022 @ HP\n",
    "#### Description : To create Power BI dashboard and provide visualized reports for weekly supply change "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e52f6",
   "metadata": {},
   "source": [
    "### Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb6886",
   "metadata": {},
   "source": [
    "1. DSP reports - need to map material numbers, forecast and supply numbers from here\n",
    "2. DSM reports - need to map consumption numbers from here \n",
    "3. WIF reports - need to map material numbers, forecast and supply numbers from here (for specific GTK model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa7409",
   "metadata": {},
   "source": [
    "### Modules and datafile settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3caad9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "#find this week\n",
    "today = datetime.date.today()\n",
    "this_monday = today - datetime.timedelta(days=today.weekday())\n",
    "#24 weeks as a planning cycle\n",
    "week24 = this_monday + datetime.timedelta(days=161)\n",
    "monday_str = this_monday.strftime(\"%m/%d/%Y\")\n",
    "monday_str2 = this_monday.strftime(\"%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce19325",
   "metadata": {},
   "source": [
    "### Use Path module to capture data source from sharepoints that synced with local computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd85d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create final df\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "#filepath\n",
    "home = str(Path.home())\n",
    "FIR_filepath = glob.glob(os.path.join(home, 'HP Inc', 'GTKSCP - BUIDM Dashboard', 'Data Source', 'DSP for Lighthouse - FIR.xlsx'))\n",
    "DSM_filepath = glob.glob(os.path.join(home, 'HP Inc', 'GTKSCP - BUIDM Dashboard', 'Data Source',  'DSM*'))\n",
    "FIR_Map_filepath = glob.glob(os.path.join(home, 'HP Inc', 'GTKSCP - DSP report', 'DT FIR - All PN List.xlsx'))\n",
    "final_filepath = os.path.join(home, 'HP Inc', 'GTKSCP - BUIDM Dashboard', 'Data Source', 'BUIDM Data input.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b7ceb",
   "metadata": {},
   "source": [
    "### Function 1 : Deal with DSM data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6368b6ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runDSM(filepath):\n",
    "\n",
    "    df_DSM = pd.read_excel(filepath, sheet_name=\"Sheet1\")\n",
    "    df_grouped = df_DSM.groupby(['Feature Type [PRD]','Feature Value [PRD]', 'Location ID', 'Key Figure'])[monday_str].sum().reset_index()\n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe58d09",
   "metadata": {},
   "source": [
    "### Function 2 : Deal with DSP data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0543713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDSP(filepath):\n",
    "\n",
    "    #import file\n",
    "    df_DSP = pd.read_excel(filepath, sheet_name= \"DSP HP view\")\n",
    "\n",
    "    #filter non-use rows\n",
    "    df_filtered = df_DSP[~df_DSP['Key Figure'].isin(['WTD Deliveries',\n",
    "                                             'Projected To Forecast HP View',\n",
    "                                             'Buffer HP View',\n",
    "                                             'Actual WOS HP View'])]\n",
    "\n",
    "    #find p/n in DSP\n",
    "    SAlist = df_filtered['Material Number'].unique()\n",
    "\n",
    "    #filter p/n in DSM\n",
    "    df_DSM = runDSM(DSM_filepath[0])\n",
    "    df_DSM = df_DSM[df_DSM['Material Number'].isin(SAlist)]\n",
    "\n",
    "    #merge\n",
    "    df_merge = pd.concat([df_filtered, df_DSM], axis=0)\n",
    "\n",
    "    #ffill/bfill NA values\n",
    "    df_merge.sort_values(by=['Material Number', 'Key Figure'], inplace=True)\n",
    "    df_merge['Ship From Loc Desc'] = df_merge['Ship From Loc Desc'].fillna(method='ffill')\n",
    "    df_merge['Ship-To Location Desc'] = df_merge['Ship-To Location Desc'].fillna(method='ffill')\n",
    "\n",
    "    #wrangling\n",
    "    cols = df_merge.select_dtypes(float).columns\n",
    "    df_merge[cols] = df_merge[cols].fillna(0)\n",
    "    df_merge[cols] = df_merge[cols].astype(int)\n",
    "    df_merge[cols] = df_merge[cols].round(0)\n",
    "\n",
    "    #drop non-used columns\n",
    "    df_merge.drop(columns=['Material Description', 'Version', 'Location ID', 'Project Code Name [CPK]'], inplace=True)\n",
    "\n",
    "    #Change KF name\n",
    "    df_merge.loc[(df_merge['Key Figure'] == 'Past Due Forecast HP View'), 'Key Figure'] = 'Forecast'\n",
    "    df_merge.loc[(df_merge['Key Figure'] == 'Forecast HP View'), 'Key Figure'] = 'Forecast'\n",
    "    df_merge.loc[(df_merge['Key Figure'] == 'Future Deliveries Total'), 'Key Figure'] = 'FD'\n",
    "    df_merge.loc[(df_merge['Key Figure'] == 'FOI Available Qty'), 'Key Figure'] = 'FOI'\n",
    "    df_merge.loc[(df_merge['Key Figure'] == 'HOI Non Visible Nettable'), 'Key Figure'] = 'HOI'\n",
    "    df_merge.loc[(df_merge['Key Figure'] == 'WMAP Consumption'), 'Key Figure'] = 'Consumption'\n",
    "\n",
    "    #Change column name\n",
    "    df_merge.rename(columns={'Feature Type [PRD]': 'Feature Type'}, inplace=True)\n",
    "    df_merge.rename(columns={'Feature Value [PRD]': 'Feature Value'}, inplace=True)\n",
    "    df_merge.rename(columns={'Ship From Loc Desc': 'Ship From'}, inplace=True)\n",
    "    df_merge.rename(columns={'Ship-To Location Desc': 'Ship To'}, inplace=True)\n",
    "\n",
    "    #unpivot date\n",
    "    df_unpivot = pd.melt(df_merge, id_vars=['Feature Type', 'Feature Value', 'Material Number', 'Ship From',\n",
    "                                            'Ship To', 'Key Figure'], var_name='Plan Week', value_name='QTY')\n",
    "\n",
    "    #date format\n",
    "    df_unpivot['Plan Week'] = pd.to_datetime(df_unpivot['Plan Week']).dt.date\n",
    "\n",
    "    #filter only FD / Forecast\n",
    "    df_unpivot = df_unpivot.loc[(df_unpivot['Plan Week'] == this_monday) | (df_unpivot['Key Figure'] == 'Forecast') | (\n",
    "                df_unpivot['Key Figure'] == 'FD')]\n",
    "\n",
    "    #group pastdue forecast & forecast\n",
    "    df_grouped = df_unpivot.groupby(['Feature Type', 'Feature Value', 'Material Number', 'Ship From',\n",
    "                                     'Ship To', 'Key Figure', 'Plan Week'])[['QTY']].sum().reset_index()\n",
    "\n",
    "    #Add ref week & category & map family\n",
    "    df_grouped['Reference Week'] = this_monday\n",
    "\n",
    "    #map platform from wif & mappingtable\n",
    "\n",
    "    # wrangling mapping tables\n",
    "    df_mapping = pd.read_excel(FIR_Map_filepath[0])\n",
    "    df_mapping['Family code'] = df_mapping['Family code'].str.strip()\n",
    "    df_mapping['SA#'] = df_mapping['SA#'].str.strip()\n",
    "    df_mapping.drop_duplicates(subset=['SA#', 'Family code'], inplace=True)\n",
    "    df_mapping = df_mapping.groupby(['SA#']).agg({'Family code': lambda x: '/'.join(x)}).reset_index()\n",
    "\n",
    "    df_grouped['Category'] = 'FIR'\n",
    "    df_grouped = df_grouped.merge(df_mapping[['SA#', 'Family code']], how=\"left\", left_on='Material Number', right_on='SA#')\n",
    "    df_grouped.drop(columns=['SA#'], inplace=True)\n",
    "    df_grouped.rename(columns={'Family code': 'Platform'}, inplace=True)\n",
    "\n",
    "\n",
    "    #date format\n",
    "    df_grouped['Plan Week'] = pd.to_datetime(df_grouped['Plan Week']).dt.date\n",
    "    df_grouped['Reference Week'] = pd.to_datetime(df_grouped['Reference Week']).dt.date\n",
    "\n",
    "    #sorting\n",
    "    df_grouped['Key Figure'] = pd.Categorical(df_grouped['Key Figure'],\n",
    "                                              categories=['Forecast', 'FD', 'FOI', 'SOI', 'HOI', 'Consumption'])\n",
    "    df_grouped.sort_values(by=[\"Material Number\", \"Ship To\", \"Reference Week\", \"Plan Week\", \"Key Figure\"], inplace=True)\n",
    "\n",
    "    #formatting\n",
    "    df_grouped['Feature Value'] = df_grouped['Feature Value'].str.title()\n",
    "    df_grouped['Ship From'] = df_grouped['Ship From'].str.title()\n",
    "    df_grouped['Ship To'] = df_grouped['Ship To'].str.title()\n",
    "    df_grouped['Platform'] = df_grouped['Platform'].str.upper()\n",
    "\n",
    "    # merge to df_final\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final, df_grouped], axis=0, ignore_index=True)\n",
    "    \n",
    "    return df_final "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b61be",
   "metadata": {},
   "source": [
    "### Function 3 : Deal with WIF data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0654b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runL10():\n",
    "\n",
    "    #WIFfilepath\n",
    "    WIF_filepath = glob.glob(os.path.join(home, 'HP Inc', 'GTKSCP - Wed DSP', 'All*'))\n",
    "\n",
    "    # find this week file\n",
    "    for i in WIF_filepath:\n",
    "        if monday_str2 in i:\n",
    "            wif_file = i\n",
    "            break\n",
    "    wif_dict = pd.read_excel(wif_file, sheet_name=None)\n",
    "\n",
    "    #concat all wif\n",
    "    all_sheets = []\n",
    "    for name, sheet in wif_dict.items():\n",
    "        sheet.columns = ['Feature Type', 'Feature Value', 'Material Number', 'Plan Week', 'FD', 'Forecast', 'Platform']\n",
    "        sheet['Ship To'] = name\n",
    "        all_sheets.append(sheet)\n",
    "\n",
    "    df_wif = pd.concat(all_sheets)\n",
    "    df_wif.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    df_wif.to_excel(home + \"\\desktop\\wif convert.xlsx\", index=False)\n",
    "\n",
    "    #wif wrangling\n",
    "    df_wif['Plan Week'] = df_wif['Plan Week'].astype('datetime64[ns]')\n",
    "    cols = df_wif.select_dtypes(float).columns\n",
    "    df_wif[cols] = df_wif[cols].fillna(0)\n",
    "    df_wif[cols] = df_wif[cols].astype(int)\n",
    "    df_wif[cols] = df_wif[cols].round(0)\n",
    "\n",
    "    cols = df_wif.select_dtypes(object).columns\n",
    "    df_wif[cols] = df_wif[cols].fillna(\"\")\n",
    "    df_wif[cols] = df_wif[cols].apply(lambda x: x.str.strip())\n",
    "    df_wif[cols] = df_wif[cols].apply(lambda x: x.str.upper())\n",
    "\n",
    "    df_wif['Plan Week'] = pd.to_datetime(df_wif['Plan Week']).dt.date\n",
    "\n",
    "    #filter\n",
    "    df_filter = df_wif.loc[(df_wif['Feature Type'] == 'BASE UNIT') | (df_wif['Feature Type'] == 'IDMECHANICAL')]\n",
    "    df_filter = df_filter.loc[df_wif['Plan Week'] <= week24 ]\n",
    "\n",
    "    #Add ref week & ship from\n",
    "    df_filter['Reference Week'] = this_monday\n",
    "    df_filter['Ship From'] = df_filter['Ship To'].values\n",
    "\n",
    "    #Finish wif wrangling\n",
    "\n",
    "    #Map ship-to from rcto pn list\n",
    "\n",
    "    #RCTO filepath\n",
    "    RCTO_filepath = glob.glob(os.path.join(home, 'HP Inc', 'GTKSCP - DSP report', 'RCTO*'))\n",
    "    #wrangling\n",
    "    df_RCTO = pd.read_excel(RCTO_filepath[0], sheet_name=\"All\")\n",
    "    df_RCTO['SA#'] = df_RCTO['SA#'].str.strip()\n",
    "    df_RCTO['Ship-to Location ID'] = df_RCTO['Ship-to Location ID'].str.strip()\n",
    "    df_RCTO.drop_duplicates(subset=['SA#', 'Ship-to Location ID'], inplace=True)\n",
    "\n",
    "\n",
    "    #Check RCTO & Add Category\n",
    "    RCTOlist = df_RCTO['SA#'].unique()\n",
    "    df_filter['Category'] = np.where(df_filter['Material Number'].isin(RCTOlist), \"RCTO\", \"L10\")\n",
    "\n",
    "    #Merge RCTO ship-to location\n",
    "    df_merge = df_filter.merge(df_RCTO[['SA#', 'Ship-to Location ID']], left_on=['Material Number'], right_on=['SA#'], how='left')\n",
    "\n",
    "    #Change Ship from\n",
    "    df_merge.loc[(df_merge['Category'] == 'L10'), 'Ship From'] = \"\"\n",
    "    df_merge.loc[(df_merge['Category'] == 'RCTO'), 'Ship To'] = df_merge.loc[(df_merge['Category'] == 'RCTO')]['Ship-to Location ID'].values\n",
    "    df_merge.drop(columns=['SA#', 'Ship-to Location ID'], inplace=True)\n",
    "\n",
    "    # Map consumption from dsm\n",
    "\n",
    "    #find p/n in DSP\n",
    "    SAlist = df_merge['Material Number'].unique()\n",
    "\n",
    "    #filter p/n in DSM\n",
    "    df_DSM = runDSM(DSM_filepath[0])\n",
    "    df_DSM = df_DSM[df_DSM['Material Number'].isin(SAlist)]\n",
    "\n",
    "    #merge_DSM\n",
    "    df_merge2 = df_merge.merge(df_DSM[['Material Number', 'Location ID', monday_str]], how=\"left\", left_on = ['Material Number', 'Ship To'], right_on = ['Material Number', 'Location ID'])\n",
    "\n",
    "    #drop non-used columns\n",
    "    df_merge2.drop(columns=['Location ID'], inplace=True)\n",
    "    df_merge2.rename(columns={monday_str: 'Consumption'}, inplace=True)\n",
    "    df_merge2['Consumption'] = df_merge2['Consumption'].fillna(0)\n",
    "    df_merge2['Consumption'] = df_merge2['Consumption'].astype(int)\n",
    "    df_merge2['Consumption'] = df_merge2['Consumption'].round(0)\n",
    "\n",
    "    df_merge2['Reference Week'] = df_merge2['Reference Week'].astype('datetime64[ns]')\n",
    "    df_merge2['Reference Week'] = pd.to_datetime(df_merge2['Reference Week']).dt.date\n",
    "\n",
    "    #unpivot\n",
    "    df_unpivot = pd.melt(df_merge2, id_vars=['Material Number', 'Feature Type', 'Feature Value', 'Ship From',\n",
    "                                             'Ship To', 'Platform', 'Reference Week', 'Plan Week', 'Category'],\n",
    "                         var_name='Key Figure', value_name='QTY')\n",
    "\n",
    "    df_unpivot = df_unpivot.loc[(df_unpivot['Plan Week'] == this_monday) | (df_unpivot['Key Figure'] == 'FD') | (\n",
    "                df_unpivot['Key Figure'] == 'Forecast')]\n",
    "\n",
    "    #Change Name\n",
    "    df_unpivot.loc[(df_unpivot['Ship To'] == '01DZ'), 'Ship To'] = 'Pegatron Chongqing'\n",
    "    df_unpivot.loc[(df_unpivot['Ship To'] == '00CV'), 'Ship To'] = 'Quanta Chongqing'\n",
    "    df_unpivot.loc[(df_unpivot['Ship To'] == '01CE'), 'Ship To'] = 'Wistron Chongqing'\n",
    "    df_unpivot.loc[(df_unpivot['Ship To'] == '0940'), 'Ship To'] = 'Compal Kun Shan'\n",
    "    df_unpivot.loc[(df_unpivot['Ship To'] == '0035'), 'Ship To'] = 'Foxconn Longhua'\n",
    "    df_unpivot.loc[(df_unpivot['Ship To'] == '01DJ'), 'Ship To'] = 'Foxconn Wuhan'\n",
    "    df_unpivot.loc[(df_unpivot['Ship To'] == '0905'), 'Ship To'] = 'Inventec Chongqing'\n",
    "    df_unpivot.loc[(df_unpivot['Ship To'] == '01DF'), 'Ship To'] = 'Inventec Taiwan'\n",
    "    df_unpivot.loc[(df_unpivot['Ship To'] == 'JP51'), 'Ship To'] = 'HP Japan'\n",
    "    df_unpivot.loc[(df_unpivot['Ship To'] == 'XG01'), 'Ship To'] = 'HP India - Flex Chennai Mfg'\n",
    "\n",
    "    df_unpivot.loc[(df_unpivot['Ship From'] == '00CV'), 'Ship From'] = 'Quanta Chongqing'\n",
    "    df_unpivot.loc[(df_unpivot['Ship From'] == '0940'), 'Ship From'] = 'Compal Kun Shan'\n",
    "    df_unpivot.loc[(df_unpivot['Ship From'] == '0905'), 'Ship From'] = 'Inventec Chongqing'\n",
    "\n",
    "    #formatting\n",
    "    df_unpivot.sort_values(by=[\"Material Number\", \"Ship To\", \"Reference Week\", \"Plan Week\", \"Key Figure\"], inplace=True)\n",
    "\n",
    "    df_unpivot['Feature Value'] = df_unpivot['Feature Value'].str.title()\n",
    "    df_unpivot['Ship From'] = df_unpivot['Ship From'].str.title()\n",
    "    df_unpivot['Ship To'] = df_unpivot['Ship To'].str.title()\n",
    "    df_unpivot['Platform'] = df_unpivot['Platform'].str.upper()\n",
    "\n",
    "    #df_unpivot.to_excel(home + \"\\desktop\\wif convert.xlsx\", index=False)\n",
    "\n",
    "    #merge to df_final\n",
    "    global df_final\n",
    "    df_final = pd.concat([df_final, df_unpivot], axis=0, ignore_index=True)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02936e1b",
   "metadata": {},
   "source": [
    "### Run files (will run different functions according to the procurement model for different components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28421662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run file\n",
    "\n",
    "#runDSM(DSM_filepath[0])\n",
    "#runL10()\n",
    "#runDSP(FIR_filepath[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b351ad",
   "metadata": {},
   "source": [
    "### Data after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb5d0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 197923 entries, 0 to 197922\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   Material Number  197923 non-null  object        \n",
      " 1   Feature Type     197923 non-null  object        \n",
      " 2   Feature Value    197923 non-null  object        \n",
      " 3   Ship From        35341 non-null   object        \n",
      " 4   Ship To          197923 non-null  object        \n",
      " 5   Platform         197923 non-null  object        \n",
      " 6   Reference Week   197923 non-null  datetime64[ns]\n",
      " 7   Plan Week        197923 non-null  datetime64[ns]\n",
      " 8   Key Figure       197923 non-null  object        \n",
      " 9   QTY              197923 non-null  int64         \n",
      " 10  Category         197923 non-null  object        \n",
      "dtypes: datetime64[ns](2), int64(1), object(8)\n",
      "memory usage: 16.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.read_excel(final_filepath, sheet_name=\"Sheet1\")\n",
    "\n",
    "df_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51751f8",
   "metadata": {},
   "source": [
    "### Export Data from MS Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c72947a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MS Access\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "\n",
    "home = str(Path.home())\n",
    "filepath = os.path.join(home, 'HP Inc', 'GTKSCP - GFX', 'Data Source',  'GFX Waterfall.accdb')\n",
    "\n",
    "try:\n",
    "    conn = pyodbc.connect(r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=%s;' %filepath)\n",
    "    print(\"Connected to MS Access\")\n",
    "\n",
    "except pyodbc.Error as e:\n",
    "    print(\"Error in connection\", e)\n",
    "\n",
    "query = \"SELECT * FROM GFX\"\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c27048b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find this week\n",
    "today = datetime.date.today()\n",
    "this_monday = today - datetime.timedelta(days=today.weekday())\n",
    "cut_date = datetime.datetime(today.year, today.month - 1, 1)\n",
    "\n",
    "#this_monday = this_monday - datetime.timedelta(days=7)\n",
    "last_monday = this_monday - datetime.timedelta(days=7)\n",
    "week24 = this_monday + datetime.timedelta(days=161)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03da59",
   "metadata": {},
   "source": [
    "### Function for final data processing and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab2d0a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_export():\n",
    "    \n",
    "    df_filter = df.loc[df['Reference Week'] >= cut_date]\n",
    "    df_filter['Ship From'] = df_filter['Ship From'].fillna('')\n",
    "    #df_filter['Reference Week'].unique()\n",
    "    \n",
    "    pivot_list = [\"Material Number\", \"Feature Type\", \"Feature Value\", \"Ship From\", \"Ship To\", \"Platform\",\"Plan Week\", \"Category\", \"Reference Week\", \"Key Figure\"]\n",
    "    pivot_list1 = [\"Material Number\", \"Feature Type\", \"Feature Value\", \"Ship From\", \"Ship To\", \"Platform\",\"Plan Week\", \"Category\", \"Reference Week\"]\n",
    "    \n",
    "    df_filter.loc[(df_filter['Key Figure'] == 'FD'), 'Key Figure'] = 'TSV'\n",
    "    df_filter.loc[(df_filter['Key Figure'] == 'FOI'), 'Key Figure'] = 'TSV'\n",
    "    df_filter.loc[(df_filter['Key Figure'] == 'SOI'), 'Key Figure'] = 'TSV'\n",
    "    df_filter.loc[(df_filter['Key Figure'] == 'HOI'), 'Key Figure'] = 'TSV'\n",
    "    \n",
    "    #group pastdue forecast & forecast\n",
    "    df_group = df_filter.groupby(pivot_list)[['QTY']].sum().reset_index()\n",
    "    \n",
    "    #date format\n",
    "    df_group['Plan Week'] = pd.to_datetime(df_group['Plan Week']).dt.date\n",
    "    df_group['Reference Week'] = pd.to_datetime(df_group['Reference Week']).dt.date\n",
    "    \n",
    "    df_pivot = df_group.pivot(index = pivot_list1,columns = 'Key Figure', values = 'QTY')\n",
    "    df_pivot = df_pivot.fillna(0)\n",
    "    df_pivot.columns.name = None\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "    \n",
    "    #print(df_pivot)\n",
    "    \n",
    "    df_filter2 = df_group.loc[(df_group['Reference Week'] == this_monday) | (df_group['Reference Week'] == last_monday)]\n",
    "    \n",
    "    # find different platform\n",
    "    df_diff = df_filter2.drop_duplicates(subset = [\"Material Number\", \"Feature Type\", \"Feature Value\", \"Ship From\",\n",
    "                                                   \"Ship To\", \"Platform\",\"Plan Week\", \"Category\"], keep=False)\n",
    "    \n",
    "    #print(df_diff)\n",
    "    \n",
    "    df_filter2.loc[(df_filter2['Key Figure'] == 'Consumption') & (df_filter2['Reference Week'] == last_monday), 'QTY'] = 0\n",
    "    \n",
    "    df_filter2 = df_filter2.set_index(pivot_list)\n",
    "    df_filter2 = df_filter2.unstack().unstack()\n",
    "    df_filter2.columns = df_filter2.columns.droplevel(0)\n",
    "    df_filter2 = df_filter2.fillna(0)\n",
    "    \n",
    "    df_subt = df_filter2.loc[:, pd.IndexSlice[:,this_monday]] - df_filter2.loc[:, pd.IndexSlice[:,last_monday]].values\n",
    "    df_demandchange = df_subt[\"Consumption\"] + df_subt[\"Forecast\"].values\n",
    "    df_supplychange = df_subt[\"Consumption\"] + df_subt[\"TSV\"].values\n",
    "    \n",
    "    #print(df_subt)\n",
    "    \n",
    "    df_demandchange.columns.name = None\n",
    "    df_demandchange = df_demandchange.reset_index()\n",
    "    df_demandchange.rename(columns={df_demandchange.columns[8]: 'Demand Change'},inplace=True)\n",
    "    \n",
    "    df_supplychange.columns.name = None\n",
    "    df_supplychange = df_supplychange.reset_index()\n",
    "    df_supplychange.rename(columns={df_supplychange.columns[8]: 'Supply Change'},inplace=True)\n",
    "    \n",
    "    df_demandchange['Reference Week'] = this_monday\n",
    "    df_supplychange['Reference Week'] = this_monday\n",
    "    \n",
    "    df_merge = df_demandchange.merge(df_supplychange, on = [\"Material Number\", \"Feature Type\", \"Feature Value\", \"Ship From\", \"Ship To\", \"Platform\",\"Plan Week\", \"Category\", \"Reference Week\"],how=\"left\")\n",
    "    \n",
    "    df_merge.loc[(df_merge['Plan Week'] == last_monday), 'Plan Week'] = this_monday\n",
    "    \n",
    "    df_group2 = df_merge.groupby(pivot_list1)[['Demand Change', 'Supply Change']].sum().reset_index()\n",
    "    \n",
    "    \n",
    "    df_merge2 = df_pivot.merge(df_group2, on = [\"Material Number\", \"Feature Type\", \"Feature Value\", \"Ship From\", \"Ship To\", \"Platform\",\"Plan Week\", \"Category\", \"Reference Week\"],how=\"left\")\n",
    "    df_merge2.sort_values(by=[\"Material Number\", \"Ship To\", \"Reference Week\", \"Plan Week\"], inplace=True)\n",
    "    df_merge2['Cumulative Supply Change'] = df_merge2.groupby(['Material Number', 'Ship To'])['Supply Change'].cumsum()\n",
    "    df_merge2['Cumulative Demand Change'] = df_merge2.groupby(['Material Number', 'Ship To'])['Demand Change'].cumsum()\n",
    "    df_merge2['TSV delta to Forecast'] = df_merge2['TSV'] - df_merge2['Forecast']\n",
    "    df_merge2['Cumulative TSV delta to Forecast'] =  df_merge2.groupby([\"Material Number\", \"Ship To\", \"Reference Week\"])['TSV delta to Forecast'].cumsum()\n",
    "    df_merge2['Cumulative TSV'] =  df_merge2.groupby([\"Material Number\", \"Ship To\", \"Reference Week\"])['TSV'].cumsum()\n",
    "    df_merge2['Cumulative Forecast'] =  df_merge2.groupby([\"Material Number\", \"Ship To\", \"Reference Week\"])['Forecast'].cumsum()\n",
    "    \n",
    "    #df_merge2['Cumulative Supply Change'] = df_merge2['Cumulative Supply Change'].fillna(0)\n",
    "    #df_merge2['Cumulative Demand Change'] = df_merge2['Cumulative Demand Change'].fillna(0)\n",
    "    \n",
    "    #reorganize\n",
    "    df_merge2 = df_merge2[['Material Number', 'Feature Type', 'Feature Value', 'Ship From', 'Ship To', 'Platform', 'Reference Week','Plan Week','Category','Consumption','Forecast','Cumulative Forecast','TSV','Cumulative TSV',\n",
    "                           'TSV delta to Forecast','Cumulative TSV delta to Forecast','Demand Change','Cumulative Demand Change','Supply Change', 'Cumulative Supply Change']]\n",
    "    \n",
    "    #df_merge2.to_excel(home+\"\\Desktop\\Data Output.xlsx\", index=False)\n",
    "    df_merge2.to_excel(home+\"\\HP Inc\\GTKSCP - BUIDM Dashboard\\Data Source\\BUIDM Data output.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce1c7586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 634472 entries, 0 to 634471\n",
      "Data columns (total 20 columns):\n",
      " #   Column                            Non-Null Count   Dtype         \n",
      "---  ------                            --------------   -----         \n",
      " 0   Material Number                   634472 non-null  object        \n",
      " 1   Feature Type                      634472 non-null  object        \n",
      " 2   Feature Value                     634472 non-null  object        \n",
      " 3   Ship From                         114334 non-null  object        \n",
      " 4   Ship To                           634472 non-null  object        \n",
      " 5   Platform                          634472 non-null  object        \n",
      " 6   Reference Week                    634472 non-null  datetime64[ns]\n",
      " 7   Plan Week                         634472 non-null  datetime64[ns]\n",
      " 8   Category                          634472 non-null  object        \n",
      " 9   Consumption                       634472 non-null  int64         \n",
      " 10  Forecast                          634472 non-null  int64         \n",
      " 11  Cumulative Forecast               634472 non-null  int64         \n",
      " 12  TSV                               634472 non-null  int64         \n",
      " 13  Cumulative TSV                    634472 non-null  int64         \n",
      " 14  TSV delta to Forecast             634472 non-null  int64         \n",
      " 15  Cumulative TSV delta to Forecast  634472 non-null  int64         \n",
      " 16  Demand Change                     96408 non-null   float64       \n",
      " 17  Cumulative Demand Change          96408 non-null   float64       \n",
      " 18  Supply Change                     96408 non-null   float64       \n",
      " 19  Cumulative Supply Change          96408 non-null   float64       \n",
      "dtypes: datetime64[ns](2), float64(4), int64(7), object(7)\n",
      "memory usage: 96.8+ MB\n"
     ]
    }
   ],
   "source": [
    "output_filepath = os.path.join(home, 'HP Inc', 'GTKSCP - BUIDM Dashboard', 'Data Source', 'BUIDM Data output.xlsx')\n",
    "\n",
    "df_final2 = pd.read_excel(output_filepath, sheet_name=\"Sheet1\")\n",
    "\n",
    "df_final2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2104680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
